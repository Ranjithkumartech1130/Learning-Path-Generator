{
  "Ranjith": {
    "username": "Ranjith",
    "email": "ranjithkumark1130@gmail.com",
    "password": "1234567",
    "profile": {
      "skills": [
        "Python",
        "SQL",
        "Data Science"
      ],
      "experience_level": "Beginner",
      "bio": "Software Developer",
      "learning_goals": [
        "Web Developement"
      ],
      "interests": [],
      "time_commitment": "6-10 hours / week",
      "learning_style": "Visual",
      "difficulty_preference": "Beginner-friendly",
      "onboarding_completed": true,
      "linkedin": "https://www.linkedin.com/in/ranjith-kumar-042574351/",
      "phone": "9791910758"
    },
    "learning_paths": [
      {
        "id": 1767169463274,
        "content": "### ðŸ“„ Detailed Learning Path\n\nWelcome to the exciting world of Machine Learning! Your journey here is exceptionally well-poised for rapid advancement. With a solid foundation in Python, SQL, and Data Science, you've already mastered the crucial prerequisites for handling data, programming logic, and deriving insights. Think of your current skills as the launchpad; Python gives you the engine, SQL provides the fuel from vast data reservoirs, and Data Science equips you with the navigation tools to understand data patterns. Machine Learning is the rocket itself, ready to take you to new frontiers where you'll build intelligent systems that learn and adapt. We're not starting from scratch; we're building on an already impressive skill set to accelerate your path to becoming a proficient Machine Learning practitioner!\n\n### ðŸ“š Recommended Open Source Resources\n\n| ðŸŽ“ Top Courses | ðŸ’» Practice Platforms | ðŸ¤ Communities |\n| :--- | :--- | :--- |\n| [Machine Learning by Andrew Ng (Coursera)](https://www.coursera.org/learn/machine-learning) | [Kaggle](https://www.kaggle.com/) | [Reddit: r/MachineLearning](https://www.reddit.com/r/MachineLearning/) |\n| [fast.ai: Practical Deep Learning for Coders](https://course.fast.ai/) | [HackerRank (Python)](https://www.hackerrank.com/domains/python) | [Stack Overflow (Machine Learning)](https://stackoverflow.com/questions/tagged/machine-learning) |\n| [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course) | [LeetCode (Python)](https://leetcode.com/) | [Towards Data Science (Medium)](https://towardsdatascience.com/) |\n\n### 3. OPEN SOURCE LEARNING RESOURCES\n\n*   **Free Online Courses:**\n    *   **Mathematics for Machine Learning:**\n        *   [Khan Academy: Linear Algebra](https://www.khanacademy.org/math/linear-algebra) - Excellent foundation for vectors, matrices, and transformations.\n        *   [Khan Academy: Multivariable Calculus](https://www.khanacademy.org/math/multivariable-calculus) - Essential for understanding optimization algorithms.\n        *   [Khan Academy: Probability and Statistics](https://www.khanacademy.org/math/statistics-probability) - Critical for understanding model uncertainty and evaluation.\n    *   **Machine Learning Fundamentals & Scikit-learn:**\n        *   [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) - Official documentation, excellent for understanding algorithms and usage.\n        *   [Python for Data Science and Machine Learning Bootcamp (YouTube)](https://www.youtube.com/playlist?list=PL-osiE80TeTs4UjLw5MM6OJGqV2Bf0WjE) - Although a bootcamp, many free resources and tutorials are available on YouTube covering these topics.\n    *   **Deep Learning Introduction:**\n        *   [Deep Learning Specialization by Andrew Ng (Coursera Audit)](https://www.coursera.org/specializations/deep-learning) - Provides a comprehensive overview of deep learning, audit available for free.\n        *   [Hugging Face Courses](https://huggingface.co/learn/nlp-course/chapter1/1) - Fantastic for diving into modern NLP and Transformers.\n\n### ðŸš€ Your Personalized Learning Path: Accelerating Towards Mastery\n\n#### 1. OVERVIEW & ASSESSMENT\n*   **Current Skill Assessment:**\n    *   **Strengths:**\n        *   **Python Proficiency:** Strong programming logic, data structures, scripting, and familiarity with core libraries (Pandas, NumPy, Matplotlib, Seaborn).\n        *   **SQL Mastery:** Efficient data querying, manipulation, and understanding of database structures. This is invaluable for sourcing and preparing data.\n        *   **Data Science Fundamentals:** Solid understanding of data cleaning, exploratory data analysis (EDA), data visualization, basic statistical inference, hypothesis testing, and potentially some elementary statistical modeling.\n    *   **Gap Analysis:**\n        *   **Core ML Algorithms:** In-depth theoretical understanding and practical implementation of supervised, unsupervised, and reinforcement learning algorithms.\n        *   **Advanced Feature Engineering:** Techniques beyond basic transformations, including domain-specific feature creation and automated methods.\n        *   **Model Evaluation & Selection:** Deeper dive into various metrics (beyond accuracy/R-squared), cross-validation strategies, bias-variance tradeoff, and hyperparameter tuning.\n        *   **Mathematical Foundations:** Specific areas of Linear Algebra, Calculus, and Probability/Statistics *as applied to ML algorithms*.\n        *   **Deep Learning:** Neural network architectures, backpropagation, and major frameworks (TensorFlow/PyTorch).\n        *   **MLOps & Deployment:** Understanding how to deploy, monitor, and maintain ML models in production.\n\n#### 2. FOUNDATION (Weeks 1-4)\n*   **Focus:** Bridging your existing Data Science knowledge to core Machine Learning concepts, solidifying essential mathematical intuition, and implementing fundamental ML algorithms using scikit-learn.\n*   **Action Items:**\n    *   **Review/Strengthen Math for ML (1 week):** Focus on the *application* of Linear Algebra (vector/matrix operations for data representation, SVD for PCA), Calculus (gradients for optimization), Probability & Statistics (distributions, hypothesis testing, Bayes' theorem for Naive Bayes). *Leverage your existing stats knowledge, but reframe it for ML context.*\n        *   *Resource:* Khan Academy math sections, \"Mathematics for Machine Learning\" book (available free online).\n    *   **Introduction to Machine Learning Workflow (1 week):** Understand the full lifecycle: problem framing, data acquisition (using your SQL!), data preprocessing, feature engineering, model selection, training, evaluation, hyperparameter tuning, and deployment.\n        *   *Resource:* Google's ML Crash Course (first few modules), \"Machine Learning with Python\" by IBM (Coursera, audit mode).\n    *   **Supervised Learning - Regression & Classification Basics (2 weeks):**\n        *   **Algorithms:** Linear Regression, Logistic Regression, K-Nearest Neighbors, Decision Trees.\n        *   **Concepts:** Cost functions, gradient descent intuition, overfitting vs. underfitting, basic evaluation metrics (MSE, R-squared, Accuracy, Precision, Recall, F1-Score).\n        *   **Hands-on:** Implement these algorithms using `scikit-learn` on well-known datasets (e.g., Boston Housing, Iris, Titanic). Practice data scaling and encoding.\n        *   *Resource:* Andrew Ng's Machine Learning Course (first few modules), Scikit-learn documentation examples.\n    *   **Mini-Project:** Pick a simple dataset (e.g., Kaggle's Titanic or House Prices - simplified) and build a basic predictive model from scratch, including data cleaning, EDA, feature engineering, model training, and evaluation. Use your SQL skills to query/prepare data if it's in a database format.\n\n#### 3. CORE SKILLS (Weeks 5-8)\n*   **Focus:** Expanding your algorithm toolkit, mastering model evaluation techniques, delving into advanced feature engineering, and understanding unsupervised learning.\n*   **Action Items:**\n    *   **Advanced Supervised Learning (3 weeks):**\n        *   **Algorithms:** Support Vector Machines (SVMs), Ensemble Methods (Random Forests, Gradient Boosting Machines like XGBoost/LightGBM).\n        *   **Concepts:** Bias-variance tradeoff, bagging, boosting, feature importance.\n        *   **Evaluation & Tuning:** Deeper dive into cross-validation (K-fold, Stratified K-fold), ROC-AUC curves, confusion matrices, hyperparameter tuning (Grid Search, Randomized Search), pipeline creation with `scikit-learn`.\n        *   **Hands-on:** Apply these algorithms to more complex datasets. Compare performance using appropriate metrics.\n        *   *Resource:* Machine Learning Mastery blog, \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" (chapter on ensemble methods).\n    *   **Unsupervised Learning (1 week):**\n        *   **Algorithms:** K-Means Clustering, DBSCAN, Principal Component Analysis (PCA) for dimensionality reduction.\n        *   **Concepts:** Cluster evaluation metrics (silhouette score), curse of dimensionality.\n        *   **Hands-on:** Use `scikit-learn` to cluster data and reduce dimensionality. Visualize the results.\n        *   *Resource:* Scikit-learn documentation on clustering and dimensionality reduction.\n    *   **Project:** Select a Kaggle \"getting started\" competition (e.g., House Prices Advanced Regression Techniques, Forest Cover Type Prediction). Focus on robust data preprocessing, advanced feature engineering, trying multiple models, and tuning hyperparameters. Your data science visualization skills will be key here!\n\n#### 4. MASTERY & PROJECTS (Weeks 9-12+)\n*   **Focus:** Introduction to Deep Learning, understanding MLOps principles, specialization in a chosen ML domain, and building a strong portfolio of end-to-end projects.\n*   **Action Items:**\n    *   **Introduction to Deep Learning (3 weeks):**\n        *   **Concepts:** Neural network architecture (input, hidden, output layers), activation functions (ReLU, Sigmoid, Softmax), loss functions, backpropagation intuition.\n        *   **Frameworks:** Get comfortable with either TensorFlow/Keras or PyTorch.\n        *   **Algorithms:** Feedforward Neural Networks (MLPs), Convolutional Neural Networks (CNNs) for image data, Recurrent Neural Networks (RNNs)/LSTMs for sequential data (basic).\n        *   **Hands-on:** Implement a basic image classifier (MNIST/CIFAR-10) with CNNs and a text classifier with MLPs/RNNs.\n        *   *Resource:* fast.ai Practical Deep Learning for Coders, Andrew Ng's Deep Learning Specialization.\n    *   **MLOps & Deployment Fundamentals (1 week):**\n        *   **Concepts:** Version control for models (DVC), basic model serving (Flask/Streamlit), understanding the need for monitoring.\n        *   **Hands-on:** Take one of your previous projects and deploy a simple web service that uses your trained model to make predictions.\n        *   *Resource:* Towards Data Science articles on MLOps, Flask/Streamlit tutorials.\n    *   **Specialization & Portfolio Building (Ongoing):**\n        *   **Choose a Niche:** Based on your interests (NLP, Computer Vision, Time Series Analysis, Recommender Systems, Reinforcement Learning), dive deeper into one area.\n        *   **End-to-End Projects:**\n            *   **Project 1 (Your Choice):** Identify a real-world problem. Use your SQL skills to gather/generate data, apply your full ML workflow, and deploy a minimum viable product. Document your process thoroughly on GitHub.\n            *   **Project 2 (Kaggle Competition):** Participate in a more advanced Kaggle competition. Focus on achieving a strong ranking and learning from top solutions.\n            *   **Project 3 (Advanced Deep Learning):** Build a project using a more complex Deep Learning architecture (e.g., fine-tuning a pre-trained Transformer for NLP, an object detection model).\n        *   **Continuous Learning:** Follow prominent researchers and companies in ML, read research papers (ArXiv), subscribe to ML newsletters, and contribute to open-source projects.\n        *   **Networking:** Engage with the ML community online and in person. Attend webinars, local meetups, and conferences. Share your projects and insights.\n\nBy following this accelerated path, leveraging your formidable Python, SQL, and Data Science skills, you'll not only learn Machine Learning but also understand how to effectively apply it to real-world problems. Good luck on your exciting journey!",
        "created_at": "2025-12-31T08:24:23.274Z"
      },
      {
        "id": 1767172311179,
        "content": "### ðŸ“„ Detailed Learning Path\n\nWelcome to the exciting world of Cloud DevOps! With your solid foundation in Python, Frontend development, Machine Learning, and Data Science, you're not just starting from scratch â€“ you're bringing a powerful arsenal of skills that will supercharge your journey. Your proficiency in Python is a massive head start for automation scripting, a cornerstone of DevOps. Your Frontend background means you intimately understand application deployment and user experience, which is crucial for building robust delivery pipelines. And your ML/Data Science expertise gives you a unique perspective on managing complex, data-intensive workloads, making you an invaluable asset in the rapidly growing MLOps space. Get ready to leverage these strengths to become a highly effective and innovative Cloud DevOps professional!\n\n### ðŸ“š Recommended Open Source Resources\n\n| ðŸŽ“ Top Courses | ðŸ’» Practice Platforms | ðŸ¤ Communities |\n| :--- | :--- | :--- |\n| [KodeKloud](https://kodekloud.com/courses/) - Excellent hands-on courses for Docker, Kubernetes, Ansible, Terraform. | [Katacoda by O'Reilly (now part of KodeKloud)](https://kodekloud.com/katacoda/) - Interactive labs for various technologies. | [r/devops](https://www.reddit.com/r/devops/) - Active Reddit community for discussions. |\n| [freeCodeCamp DevOps Curriculum](https://www.youtube.com/c/freecodecamp) - Search for \"DevOps\" or specific tools on their YouTube channel. | [HackerRank](https://www.hackerrank.com/) - Coding challenges, some specific to cloud/ops tasks. | [CNCF Slack Workspace](https://slack.cncf.io/) - For Kubernetes and cloud-native discussions. |\n| [ExamPro (AWS/Azure/GCP)](https://www.youtube.com/c/ExamPro) - Free certification prep on YouTube. | [A Cloud Guru Playground](https://acloudguru.com/pricing) - (Paid, but invaluable) Sandboxes for AWS, Azure, GCP. | [Stack Overflow](https://stackoverflow.com/) - Q&A for all tech issues. |\n| [Google Cloud Skills Boost](https://cloud.google.com/training) - Free labs and quests on GCP. | [Cloud Provider Free Tiers](https://aws.amazon.com/free/, https://azure.microsoft.com/en-us/free/, https://cloud.google.com/free/) - Essential for hands-on practice. | [DevOps.com](https://devops.com/) - News, articles, and community forums. |\n\n### 3. OPEN SOURCE LEARNING RESOURCES\n\n*   **Linux Fundamentals & Shell Scripting:**\n    *   [The Linux Command Line (Book)](http://linuxcommand.org/tlcl.php) - Free online book, excellent for beginners.\n    *   [Linux Journey](https://linuxjourney.com/) - Interactive step-by-step tutorial for Linux.\n    *   [Shell Scripting Tutorial](https://ryanstutorials.net/bash/index.php) - Comprehensive guide to Bash scripting.\n*   **Git & Version Control:**\n    *   [Pro Git Book](https://git-scm.com/book/en/v2) - The official Git book, free and comprehensive.\n    *   [GitHub Learning Lab](https://lab.github.com/) - Interactive tutorials directly on GitHub.\n*   **Cloud Computing Fundamentals (Choose one primary provider for initial deep dive, e.g., AWS):**\n    *   **AWS:**\n        *   [AWS Cloud Practitioner Essentials (Digital Course)](https://aws.amazon.com/training/digital/aws-cloud-practitioner-essentials/) - Free, foundational understanding of AWS.\n        *   [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/) - Best practices for designing cloud systems.\n    *   **Azure:**\n        *   [Azure Fundamentals (AZ-900) Learning Path](https://docs.microsoft.com/en-us/learn/certifications/azure-fundamentals/) - Free modules from Microsoft Learn.\n    *   **GCP:**\n        *   [Google Cloud Digital Leader Learning Path](https://cloud.google.com/certification/cloud-digital-leader) - Free resources for GCP basics.\n*   **Docker & Containerization:**\n    *   [Docker Documentation](https://docs.docker.com/) - Official guide, excellent for understanding concepts and commands.\n    *   [Traversy Media Docker Crash Course](https://www.youtube.com/watch?v=fqMkwU13p7c) - Great video tutorial for getting started.\n*   **Kubernetes & Orchestration:**\n    *   [Kubernetes Documentation](https://kubernetes.io/docs/home/) - Official source for all Kubernetes info.\n    *   [Kubernetes Crash Course for Absolute Beginners](https://www.youtube.com/watch?v=X48VuFYp0do) - TechWorld with Nana (YouTube).\n*   **Infrastructure as Code (IaC) - Terraform:**\n    *   [Terraform Documentation](https://www.terraform.io/docs/index.html) - Official guides and tutorials.\n    *   [HashiCorp Learn](https://learn.hashicorp.com/terraform) - Hands-on labs for Terraform.\n*   **CI/CD (Continuous Integration/Continuous Delivery):**\n    *   [GitHub Actions Documentation](https://docs.github.com/en/actions) - Excellent for integrating CI/CD into your existing GitHub workflows.\n    *   [Jenkins Documentation](https://www.jenkins.io/doc/) - A widely used open-source automation server.\n*   **Monitoring & Logging:**\n    *   [Prometheus Documentation](https://prometheus.io/docs/introduction/overview/) - Open-source monitoring system.\n    *   [Grafana Documentation](https://grafana.com/docs/grafana/latest/) - Open-source analytics and monitoring visualization.\n\n### ðŸš€ Your Personalized Learning Path: Accelerating Towards Mastery\n\n#### 1. OVERVIEW & ASSESSMENT\n*   **Current Skill Assessment:**\n    *   **Strengths:**\n        *   **Python:** Strong scripting capabilities, ideal for automation, cloud SDKs (boto3, Azure SDK, Google Cloud client libraries), and building custom tools.\n        *   **Frontend:** Understanding of web application architecture, deployment needs, static site hosting, APIs, and user experience, which informs CI/CD pipeline design.\n        *   **Machine Learning/Data Science:** Experience with data pipelines, model deployment, resource requirements for ML workloads, Dockerizing ML models, and MLOps concepts (model versioning, reproducibility).\n        *   **Problem-Solving:** Analytical skills developed in ML/DS are highly transferable to debugging complex distributed systems.\n    *   **Gap Analysis:**\n        *   **Operating Systems:** Deeper understanding of Linux administration, file systems, permissions, process management.\n        *   **Networking:** Core concepts (TCP/IP, DNS, HTTP/S, firewalls, VPCs) from an infrastructure perspective.\n        *   **Cloud Provider Specifics:** In-depth knowledge of IaaS, PaaS, SaaS offerings, security, and networking within a chosen cloud (e.g., AWS, Azure, GCP).\n        *   **DevOps Toolchain:** Familiarity with core tools like Docker, Kubernetes, Terraform, CI/CD platforms (e.g., Jenkins, GitHub Actions).\n        *   **Infrastructure as Code (IaC):** Principles and practical application of tools like Terraform or CloudFormation.\n        *   **Monitoring & Logging:** Setting up and interpreting system and application metrics, logs, and alerts.\n        *   **Security:** Cloud security best practices, IAM, network security groups, compliance.\n\n#### 2. FOUNDATION (Weeks 1-4)\n*   **Focus:** Bridging core OS and networking gaps, mastering Git, and getting acquainted with foundational cloud concepts. Leveraging Python for initial automation.\n*   **Action Items:**\n    *   **Linux CLI Mastery:** Spend significant time learning essential Linux commands, file system navigation, permissions, and process management. Practice shell scripting basics.\n        *   *Leverage Python:* Write small Python scripts that execute shell commands or manage files and directories, deepening your understanding of OS interaction.\n    *   **Networking Fundamentals:** Understand TCP/IP, DNS, HTTP/S, common ports, and how firewalls work. This is crucial for cloud security and connectivity.\n    *   **Git & GitHub Proficiency:** Master branching, merging, pull requests, and Git workflows. This is the backbone of collaborative DevOps.\n    *   **Cloud Computing Introduction (Choose AWS as primary focus for now):**\n        *   Complete the AWS Cloud Practitioner Essentials course.\n        *   Set up an AWS Free Tier account and launch an EC2 instance, create an S3 bucket, understand IAM users/roles.\n        *   Explore basic networking concepts in AWS (VPC, Subnets, Security Groups).\n        *   *Leverage Python:* Use `boto3` (AWS SDK for Python) to programmatically interact with AWS services â€“ e.g., list S3 buckets, start/stop EC2 instances. This immediately connects your Python skills to cloud operations.\n\n#### 3. CORE SKILLS (Weeks 5-8)\n*   **Focus:** Containerization, Introduction to CI/CD, and Infrastructure as Code. Begin building pipelines.\n*   **Action Items:**\n    *   **Docker & Containerization:**\n        *   Learn Docker basics: `Dockerfile`, images, containers, volumes, networks.\n        *   Containerize a simple Python Flask/Django application (from your ML/Frontend background) and a simple Nginx web server.\n        *   Understand container orchestration concepts (without diving into Kubernetes yet).\n        *   *Leverage Python:* Create a `Dockerfile` for one of your existing ML models or a frontend application. Write Python scripts to automate Docker build/push operations.\n    *   **Introduction to CI/CD:**\n        *   Understand the principles of Continuous Integration and Continuous Delivery.\n        *   Set up a basic CI pipeline using GitHub Actions (as you're likely familiar with GitHub) to:\n            *   Lint your Python code.\n            *   Run unit tests for your Python application.\n            *   Build a Docker image.\n            *   *Leverage Frontend:* Integrate a build step for a simple frontend project (e.g., React build) into a CI pipeline.\n    *   **Infrastructure as Code (IaC) - Terraform:**\n        *   Learn Terraform fundamentals: providers, resources, variables, outputs.\n        *   Write Terraform configurations to provision basic AWS resources: a VPC, subnets, EC2 instances, security groups.\n        *   Practice managing infrastructure changes (apply, plan, destroy).\n        *   *Leverage Python:* Explore how to use Python to generate Terraform configuration files or parse Terraform state files for reporting.\n\n#### 4. MASTERY & PROJECTS (Weeks 9-16+)\n*   **Focus:** Orchestration (Kubernetes), Advanced CI/CD, Monitoring & Logging, Cloud Security, and building end-to-end projects. Specialize towards MLOps/DataOps.\n*   **Action Items:**\n    *   **Kubernetes Fundamentals:**\n        *   Understand Kubernetes architecture: Master, Nodes, Pods, Deployments, Services, Namespaces.\n        *   Deploy a simple application (e.g., your containerized Python app or Frontend app) to `minikube` or `kind` locally.\n        *   Learn to write basic YAML manifests for Deployments and Services.\n        *   *Leverage ML/Data Science:* Deploy a simple REST API wrapped ML model onto Kubernetes. Understand how to manage resources (CPU/memory requests/limits) for ML workloads.\n    *   **Advanced CI/CD & GitOps:**\n        *   Extend your CI pipeline to include CD: automatically deploy your application to a staging environment (e.g., using GitHub Actions to deploy to AWS EC2/ECS/EKS).\n        *   Explore concepts like blue/green deployments, canary releases.\n        *   Introduce static analysis (security scanning) into your pipeline.\n        *   Understand GitOps principles â€“ using Git as the single source of truth for declarative infrastructure and applications.\n    *   **Monitoring & Logging:**\n        *   Set up basic monitoring for your deployed applications and infrastructure using Prometheus and Grafana (or cloud-native tools like CloudWatch/Stackdriver).\n        *   Implement centralized logging using the ELK stack (Elasticsearch, Logstash, Kibana) or cloud-native solutions.\n        *   Configure alerts for critical metrics.\n    *   **Cloud Security Fundamentals:**\n        *   Deep dive into IAM best practices (least privilege, MFA).\n        *   Understand network security (Security Groups, Network ACLs, VPNs).\n        *   Learn about encryption at rest and in transit.\n        *   Implement secrets management (e.g., AWS Secrets Manager, HashiCorp Vault).\n    *   **End-to-End Project (Your Capstone):**\n        *   **Project Idea:** Build a data-driven application (leveraging your ML/Data Science skills).\n            *   **Frontend:** A simple web interface for interaction.\n            *   **Backend:** A Python Flask/FastAPI service hosting an ML model endpoint.\n            *   **Data Storage:** S3/GCS bucket for data, or a managed database.\n            *   **Infrastructure:** Provisioned entirely with Terraform (VPC, EC2/ECS/EKS for the application, databases, monitoring components).\n            *   **CI/CD:** Implement a full pipeline using GitHub Actions to:\n                *   Build and test the frontend.\n                *   Build, test, and containerize the Python backend/ML model.\n                *   Deploy the infrastructure via Terraform.\n                *   Deploy the application (frontend and backend) to a cloud environment (e.g., AWS ECS Fargate or EKS).\n            *   **Monitoring:** Set up Prometheus/Grafana or CloudWatch to monitor application health and ML model performance.\n            *   *Special MLOps Twist:* Include steps for model versioning, retraining, and redeploying the model via the CI/CD pipeline.\n    *   **Continuous Learning & Specialization:**\n        *   Explore Serverless computing (AWS Lambda, Azure Functions, GCP Cloud Functions) and its use cases.\n        *   Dive deeper into specific areas: network engineering, security, data engineering (leveraging your DS background to focus on building robust data pipelines).\n        *   Participate in open-source DevOps projects.\n        *   Stay updated with new tools and best practices through blogs, conferences, and communities.\n\nThis path is designed to be iterative and hands-on. Your existing skills are not just background knowledge; they are active tools you'll use at every stage to build, automate, and innovate in the Cloud DevOps landscape. Good luck!",
        "created_at": "2025-12-31T09:11:51.179Z"
      }
    ],
    "tasks": [
      {
        "title": "Simple House Price Prediction",
        "description": "Your first task is to build a basic machine learning model to predict house prices. You'll work with a synthetic dataset containing features like `square_footage` and `number_of_bedrooms`.\n\n**Steps:**\n1.  **Generate Data:** A synthetic dataset is provided within the starter code.\n2.  **Split Data:** Separate the features (X) from the target (y, which is 'price'). Then, split the data into training and testing sets (e.g., 80% train, 20% test).\n3.  **Train Model:** Initialize and train a `LinearRegression` model using the training data.\n4.  **Make Predictions:** Use the trained model to predict prices on the test set.\n5.  **Evaluate:** Calculate the Mean Absolute Error (MAE) between the actual and predicted prices on the test set.",
        "starter_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\ndef predict_house_prices():\n    # --- Task 1: Generate Synthetic Data (Do not modify this part) ---\n    np.random.seed(42)\n    n_samples = 100\n    square_footage = np.random.randint(800, 3000, n_samples)\n    number_of_bedrooms = np.random.randint(1, 5, n_samples)\n    # Price is a linear combination of features with some noise\n    price = 50 * square_footage + 20000 * number_of_bedrooms + np.random.normal(0, 20000, n_samples)\n    price = np.abs(price) # Ensure prices are positive\n\n    data = pd.DataFrame({\n        'square_footage': square_footage,\n        'number_of_bedrooms': number_of_bedrooms,\n        'price': price\n    })\n\n    # --- Your code starts here ---\n    # 2. Split data into features (X) and target (y)\n    X = data[['square_footage', 'number_of_bedrooms']]\n    y = data['price']\n\n    # 3. Split X and y into training and testing sets (80% train, 20% test, random_state=42)\n    X_train, X_test, y_train, y_test = ..., ..., ..., ...\n\n    # 4. Initialize and train a LinearRegression model\n    model = ...\n    model.fit(...)\n\n    # 5. Make predictions on the test set\n    y_pred = ...\n\n    # 6. Calculate and return the Mean Absolute Error (MAE)\n    mae = ...\n    return mae\n\nif __name__ == '__main__':\n    error = predict_house_prices()\n    print(f\"Mean Absolute Error: {error:.2f}\")",
        "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\ndef predict_house_prices():\n    # --- Task 1: Generate Synthetic Data (Do not modify this part) ---\n    np.random.seed(42)\n    n_samples = 100\n    square_footage = np.random.randint(800, 3000, n_samples)\n    number_of_bedrooms = np.random.randint(1, 5, n_samples)\n    # Price is a linear combination of features with some noise\n    price = 50 * square_footage + 20000 * number_of_bedrooms + np.random.normal(0, 20000, n_samples)\n    price = np.abs(price) # Ensure prices are positive\n\n    data = pd.DataFrame({\n        'square_footage': square_footage,\n        'number_of_bedrooms': number_of_bedrooms,\n        'price': price\n    })\n\n    # --- Your code starts here ---\n    # 2. Split data into features (X) and target (y)\n    X = data[['square_footage', 'number_of_bedrooms']]\n    y = data['price']\n\n    # 3. Split X and y into training and testing sets (80% train, 20% test, random_state=42)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # 4. Initialize and train a LinearRegression model\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n\n    # 5. Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # 6. Calculate and return the Mean Absolute Error (MAE)\n    mae = mean_absolute_error(y_test, y_pred)\n    return mae\n\nif __name__ == '__main__':\n    error = predict_house_prices()\n    print(f\"Mean Absolute Error: {error:.2f}\")",
        "id": "17671695126490ykpm71ox",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:12.649Z"
      },
      {
        "title": "Customer Churn Classifier",
        "description": "This task involves building a binary classification model to predict customer churn. A key challenge will be handling categorical features before training your model. You'll use a synthetic dataset with features like `monthly_charges`, `data_usage`, and `contract_type`.\n\n**Steps:**\n1.  **Generate Data:** A synthetic dataset is provided.\n2.  **Encode Categorical Feature:** Convert the `contract_type` categorical column into numerical format using one-hot encoding (Hint: `pd.get_dummies()` is a straightforward way to do this for beginners).\n3.  **Split Data:** Separate features (X) from the target (y, which is 'churn'). Split the processed data into training and testing sets. Remember to use `stratify=y` for classification tasks to maintain the proportion of classes in both train and test sets.\n4.  **Train Model:** Initialize and train a `LogisticRegression` model.\n5.  **Make Predictions:** Use the trained model to predict churn on the test set.\n6.  **Evaluate:** Calculate and return the accuracy score of your model.",
        "starter_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef predict_customer_churn():\n    # --- Task 1: Generate Synthetic Data (Do not modify this part) ---\n    np.random.seed(42)\n    n_samples = 150\n    monthly_charges = np.random.uniform(30, 120, n_samples)\n    data_usage = np.random.uniform(5, 50, n_samples)\n    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples)\n    \n    # Simulate churn based on charges, data usage, and contract type\n    # Month-to-month customers are more likely to churn\n    churn_prob = (0.02 * monthly_charges - 0.01 * data_usage + \n                  np.where(contract_type == 'Month-to-month', 0.5, 0.1) + \n                  np.random.normal(0, 0.1, n_samples))\n    churn = (churn_prob > 0.5).astype(int)\n\n    data = pd.DataFrame({\n        'monthly_charges': monthly_charges,\n        'data_usage': data_usage,\n        'contract_type': contract_type,\n        'churn': churn\n    })\n\n    # --- Your code starts here ---\n    # 2. Perform one-hot encoding on 'contract_type'.\n    #    Use pd.get_dummies() for simplicity. Remember to drop the original column.\n    data_encoded = ...\n\n    # 3. Split data into features (X) and target (y)\n    #    Ensure 'churn' is not in X\n    X = ...\n    y = data_encoded['churn']\n\n    # 4. Split X and y into training and testing sets (80% train, 20% test, random_state=42, stratify=y)\n    #    Use stratify=y for classification tasks to maintain class balance\n    X_train, X_test, y_train, y_test = ..., ..., ..., ...\n\n    # 5. Initialize and train a LogisticRegression model (solver='liblinear', random_state=42)\n    #    solver='liblinear' is a good default for small datasets and binary classification\n    model = ...\n    model.fit(...)\n\n    # 6. Make predictions on the test set\n    y_pred = ...\n\n    # 7. Calculate and return the accuracy score\n    accuracy = ...\n    return accuracy\n\nif __name__ == '__main__':\n    acc = predict_customer_churn()\n    print(f\"Model Accuracy: {acc:.2f}\")",
        "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef predict_customer_churn():\n    # --- Task 1: Generate Synthetic Data (Do not modify this part) ---\n    np.random.seed(42)\n    n_samples = 150\n    monthly_charges = np.random.uniform(30, 120, n_samples)\n    data_usage = np.random.uniform(5, 50, n_samples)\n    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples)\n    \n    # Simulate churn based on charges, data usage, and contract type\n    # Month-to-month customers are more likely to churn\n    churn_prob = (0.02 * monthly_charges - 0.01 * data_usage + \n                  np.where(contract_type == 'Month-to-month', 0.5, 0.1) + \n                  np.random.normal(0, 0.1, n_samples))\n    churn = (churn_prob > 0.5).astype(int)\n\n    data = pd.DataFrame({\n        'monthly_charges': monthly_charges,\n        'data_usage': data_usage,\n        'contract_type': contract_type,\n        'churn': churn\n    })\n\n    # --- Your code starts here ---\n    # 2. Perform one-hot encoding on 'contract_type'.\n    #    Use pd.get_dummies() for simplicity. Remember to drop the original column.\n    data_encoded = pd.get_dummies(data, columns=['contract_type'], drop_first=True)\n\n    # 3. Split data into features (X) and target (y)\n    #    Ensure 'churn' is not in X\n    X = data_encoded.drop('churn', axis=1)\n    y = data_encoded['churn']\n\n    # 4. Split X and y into training and testing sets (80% train, 20% test, random_state=42, stratify=y)\n    #    Use stratify=y for classification tasks to maintain class balance\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n    # 5. Initialize and train a LogisticRegression model (solver='liblinear', random_state=42)\n    #    solver='liblinear' is a good default for small datasets and binary classification\n    model = LogisticRegression(solver='liblinear', random_state=42)\n    model.fit(X_train, y_train)\n\n    # 6. Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # 7. Calculate and return the accuracy score\n    accuracy = accuracy_score(y_test, y_pred)\n    return accuracy\n\nif __name__ == '__main__':\n    acc = predict_customer_churn()\n    print(f\"Model Accuracy: {acc:.2f}\")",
        "id": "1767169512649je1xjvro1",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:12.649Z"
      },
      {
        "title": "Optimize Churn Predictor with Decision Trees and Cross-Validation",
        "description": "Building on the previous churn prediction task, you'll now use a `DecisionTreeClassifier` and focus on a more robust evaluation method: cross-validation. You'll also explore how a simple hyperparameter (`max_depth`) can affect model performance.\n\n**Steps:**\n1.  **Prepare Data:** Use the same synthetic customer churn data and one-hot encoding as in Task 2.\n2.  **Create Evaluation Function:** Define a function `evaluate_decision_tree_with_cv(X, y, max_depth)`.\n3.  **Train & Cross-Validate:** Inside this function:\n    *   Initialize a `DecisionTreeClassifier` with the given `max_depth` (and `random_state=42`).\n    *   Use `sklearn.model_selection.cross_val_score` to evaluate the model's accuracy using 5-fold cross-validation (`cv=5`, `scoring='accuracy'`).\n    *   Return the *mean* accuracy score across all folds.\n4.  **Experiment:** Call your `evaluate_decision_tree_with_cv` function for different `max_depth` values (e.g., 3, 5, 10) to observe how this parameter influences the model's performance. Print the results.",
        "starter_code": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef optimize_churn_predictor_dt():\n    # --- Task 1: Generate Synthetic Data (Same as Task 2, do not modify) ---\n    np.random.seed(42)\n    n_samples = 150\n    monthly_charges = np.random.uniform(30, 120, n_samples)\n    data_usage = np.random.uniform(5, 50, n_samples)\n    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples)\n    \n    churn_prob = (0.02 * monthly_charges - 0.01 * data_usage + \n                  np.where(contract_type == 'Month-to-month', 0.5, 0.1) + \n                  np.random.normal(0, 0.1, n_samples))\n    churn = (churn_prob > 0.5).astype(int)\n\n    data = pd.DataFrame({\n        'monthly_charges': monthly_charges,\n        'data_usage': data_usage,\n        'contract_type': contract_type,\n        'churn': churn\n    })\n\n    # --- Task 2: One-Hot Encode (Same as Task 2, do not modify) ---\n    data_encoded = pd.get_dummies(data, columns=['contract_type'], drop_first=True)\n    X = data_encoded.drop('churn', axis=1)\n    y = data_encoded['churn']\n\n    # --- Your code starts here ---\n    # 3. Define a function to evaluate Decision Tree with cross-validation\n    def evaluate_decision_tree_with_cv(X, y, max_depth):\n        # Initialize DecisionTreeClassifier with the given max_depth and random_state=42\n        model = ...\n\n        # Use cross_val_score to evaluate model accuracy with 5-fold cross-validation\n        # scoring='accuracy'\n        scores = ...\n        \n        # Return the mean accuracy\n        return np.mean(scores)\n\n    # 4. Evaluate the model for different max_depth values\n    depths_to_test = [3, 5, 10]\n    results = {}\n    for depth in depths_to_test:\n        mean_accuracy = evaluate_decision_tree_with_cv(X, y, depth)\n        results[depth] = mean_accuracy\n    \n    return results\n\nif __name__ == '__main__':\n    evaluation_results = optimize_churn_predictor_dt()\n    for depth, accuracy in evaluation_results.items():\n        print(f\"Decision Tree with max_depth={depth}: Mean Cross-Validation Accuracy = {accuracy:.2f}\")",
        "solution": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef optimize_churn_predictor_dt():\n    # --- Task 1: Generate Synthetic Data (Same as Task 2, do not modify) ---\n    np.random.seed(42)\n    n_samples = 150\n    monthly_charges = np.random.uniform(30, 120, n_samples)\n    data_usage = np.random.uniform(5, 50, n_samples)\n    contract_type = np.random.choice(['Month-to-month', 'One year', 'Two year'], n_samples)\n    \n    churn_prob = (0.02 * monthly_charges - 0.01 * data_usage + \n                  np.where(contract_type == 'Month-to-month', 0.5, 0.1) + \n                  np.random.normal(0, 0.1, n_samples))\n    churn = (churn_prob > 0.5).astype(int)\n\n    data = pd.DataFrame({\n        'monthly_charges': monthly_charges,\n        'data_usage': data_usage,\n        'contract_type': contract_type,\n        'churn': churn\n    })\n\n    # --- Task 2: One-Hot Encode (Same as Task 2, do not modify) ---\n    data_encoded = pd.get_dummies(data, columns=['contract_type'], drop_first=True)\n    X = data_encoded.drop('churn', axis=1)\n    y = data_encoded['churn']\n\n    # --- Your code starts here ---\n    # 3. Define a function to evaluate Decision Tree with cross-validation\n    def evaluate_decision_tree_with_cv(X, y, max_depth):\n        # Initialize DecisionTreeClassifier with the given max_depth and random_state=42\n        model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n\n        # Use cross_val_score to evaluate model accuracy with 5-fold cross-validation\n        # scoring='accuracy'\n        scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n        \n        # Return the mean accuracy\n        return np.mean(scores)\n\n    # 4. Evaluate the model for different max_depth values\n    depths_to_test = [3, 5, 10]\n    results = {}\n    for depth in depths_to_test:\n        mean_accuracy = evaluate_decision_tree_with_cv(X, y, depth)\n        results[depth] = mean_accuracy\n    \n    return results\n\nif __name__ == '__main__':\n    evaluation_results = optimize_churn_predictor_dt()\n    for depth, accuracy in evaluation_results.items():\n        print(f\"Decision Tree with max_depth={depth}: Mean Cross-Validation Accuracy = {accuracy:.2f}\")",
        "id": "1767169512649etxgu8og5",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:12.649Z"
      },
      {
        "title": "Basic Data Cleaning for ML",
        "description": "You are given a pandas DataFrame with numerical and categorical columns. Some numerical columns might have missing values (represented as NaN), and you need to encode one specific categorical column. Your task is to:\n1.  Fill any missing numerical values in the 'Age' column with the mean of that column.\n2.  Apply one-hot encoding to the 'City' column. Ensure the original 'City' column is dropped after encoding to avoid redundancy.\n3.  Return the processed DataFrame.",
        "starter_code": "import pandas as pd\nimport numpy as np\n\ndef preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Processes the input DataFrame by filling missing 'Age' values with the mean\n    and applying one-hot encoding to the 'City' column.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        pd.DataFrame: The processed DataFrame.\n    \"\"\"\n    # Your code here\n    pass\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    data = {\n        'ID': [1, 2, 3, 4, 5],\n        'Age': [25, 30, np.nan, 35, 40],\n        'City': ['New York', 'London', 'New York', 'Paris', 'London'],\n        'Salary': [50000, 60000, 55000, 70000, 62000]\n    }\n    df = pd.DataFrame(data)\n    print(\"Original DataFrame:\")\n    print(df)\n    processed_df = preprocess_data(df.copy()) # Use .copy() to avoid modifying original df\n    print(\"\\nProcessed DataFrame:\")\n    print(processed_df)",
        "solution": "import pandas as pd\nimport numpy as np\n\ndef preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Processes the input DataFrame by filling missing 'Age' values with the mean\n    and applying one-hot encoding to the 'City' column.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        pd.DataFrame: The processed DataFrame.\n    \"\"\"\n    # 1. Fill missing 'Age' values with the mean\n    df['Age'].fillna(df['Age'].mean(), inplace=True)\n\n    # 2. Apply one-hot encoding to the 'City' column\n    # pd.get_dummies returns a new DataFrame with the encoded columns\n    df_encoded_city = pd.get_dummies(df['City'], prefix='City', dtype=int)\n\n    # Concatenate the new encoded columns with the original DataFrame\n    # and drop the original 'City' column\n    df = pd.concat([df, df_encoded_city], axis=1)\n    df.drop('City', axis=1, inplace=True)\n\n    return df\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    data = {\n        'ID': [1, 2, 3, 4, 5],\n        'Age': [25, 30, np.nan, 35, 40],\n        'City': ['New York', 'London', 'New York', 'Paris', 'London'],\n        'Salary': [50000, 60000, 55000, 70000, 62000]\n    }\n    df = pd.DataFrame(data)\n    print(\"Original DataFrame:\")\n    print(df)\n    processed_df = preprocess_data(df.copy())\n    print(\"\\nProcessed DataFrame:\")\n    print(processed_df)",
        "id": "17671695458914rd4u8hci",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:45.891Z"
      },
      {
        "title": "First ML Model: Predicting Iris Species with KNN",
        "description": "The Iris dataset is a classic for classification. It contains measurements of iris flowers and their corresponding species. Your goal is to:\n1.  Load the Iris dataset using `sklearn.datasets.load_iris()`.\n2.  Extract the features (X) and target (y).\n3.  Initialize a K-Nearest Neighbors (KNN) classifier with `n_neighbors=3`.\n4.  Train the KNN model using all available data.\n5.  Predict the species for a given new sample (a list of 4 float values representing sepal length, sepal width, petal length, petal width).\n6.  Return the predicted class (an integer representing the species).",
        "starter_code": "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef predict_iris_species(new_sample: list) -> int:\n    \"\"\"\n    Trains a KNN model on the Iris dataset and predicts the species\n    for a new sample.\n\n    Args:\n        new_sample (list): A list of 4 float values [sepal_length, sepal_width, petal_length, petal_width].\n\n    Returns:\n        int: The predicted species class (0, 1, or 2).\n    \"\"\"\n    # Load the Iris dataset\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Your code here\n    pass\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    # Example of a new iris flower measurement\n    sample = [5.1, 3.5, 1.4, 0.2] # This is a Setosa\n    predicted_species = predict_iris_species(sample)\n    print(f\"The predicted species for sample {sample} is: {predicted_species}\")\n    # To get the actual species name: iris.target_names[predicted_species]\n    iris = load_iris()\n    print(f\"Which corresponds to: {iris.target_names[predicted_species]}\")\n\n    sample2 = [6.0, 2.2, 5.0, 1.5] # This is likely Versicolor\n    predicted_species2 = predict_iris_species(sample2)\n    print(f\"The predicted species for sample {sample2} is: {predicted_species2}\")\n    print(f\"Which corresponds to: {iris.target_names[predicted_species2]}\")",
        "solution": "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef predict_iris_species(new_sample: list) -> int:\n    \"\"\"\n    Trains a KNN model on the Iris dataset and predicts the species\n    for a new sample.\n\n    Args:\n        new_sample (list): A list of 4 float values [sepal_length, sepal_width, petal_length, petal_width].\n\n    Returns:\n        int: The predicted species class (0, 1, or 2).\n    \"\"\"\n    # Load the Iris dataset\n    iris = load_iris()\n    X, y = iris.data, iris.target\n\n    # Initialize a K-Nearest Neighbors classifier\n    knn = KNeighborsClassifier(n_neighbors=3)\n\n    # Train the model\n    knn.fit(X, y)\n\n    # Predict the species for the new sample\n    # The model expects a 2D array, even for a single sample\n    predicted_class = knn.predict(np.array(new_sample).reshape(1, -1))[0]\n\n    return predicted_class\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    # Example of a new iris flower measurement\n    sample = [5.1, 3.5, 1.4, 0.2] # This is a Setosa\n    predicted_species = predict_iris_species(sample)\n    print(f\"The predicted species for sample {sample} is: {predicted_species}\")\n    # To get the actual species name: iris.target_names[predicted_species]\n    iris = load_iris()\n    print(f\"Which corresponds to: {iris.target_names[predicted_species]}\")\n\n    sample2 = [6.0, 2.2, 5.0, 1.5] # This is likely Versicolor\n    predicted_species2 = predict_iris_species(sample2)\n    print(f\"The predicted species for sample {sample2} is: {predicted_species2}\")\n    print(f\"Which corresponds to: {iris.target_names[predicted_species2]}\")",
        "id": "1767169545891rk892tpef",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:45.891Z"
      },
      {
        "title": "Evaluating Model Performance: Train-Test Split & Accuracy",
        "description": "To properly evaluate a machine learning model, it's crucial to test it on data it hasn't seen during training. For the Iris dataset:\n1.  Load the Iris dataset and split it into features (X) and target (y).\n2.  Split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. Use `test_size=0.3`, `random_state=42`, and `stratify=y` (important for classification tasks to maintain class proportions).\n3.  Initialize a Logistic Regression model (with `random_state=42` for reproducibility and `max_iter=200` to ensure convergence).\n4.  Train the model on the training data (`X_train`, `y_train`).\n5.  Make predictions on the test set (`X_test`).\n6.  Calculate and return the accuracy score of the model on the test set.",
        "starter_code": "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef evaluate_iris_model(X: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"\n    Splits the Iris dataset, trains a Logistic Regression model,\n    and evaluates its accuracy on the test set.\n\n    Args:\n        X (np.ndarray): The feature matrix of the dataset.\n        y (np.ndarray): The target vector of the dataset.\n\n    Returns:\n        float: The accuracy score of the model on the test set.\n    \"\"\"\n    # Your code here\n    pass\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    accuracy = evaluate_iris_model(X, y)\n    print(f\"Model accuracy on the test set: {accuracy:.4f}\")",
        "solution": "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef evaluate_iris_model(X: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"\n    Splits the Iris dataset, trains a Logistic Regression model,\n    and evaluates its accuracy on the test set.\n\n    Args:\n        X (np.ndarray): The feature matrix of the dataset.\n        y (np.ndarray): The target vector of the dataset.\n\n    Returns:\n        float: The accuracy score of the model on the test set.\n    \"\"\"\n    # 1. Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.3, random_state=42, stratify=y \n    )\n\n    # 2. Initialize a Logistic Regression model\n    # max_iter is often increased for convergence warnings with default settings with some solvers\n    model = LogisticRegression(random_state=42, max_iter=200)\n\n    # 3. Train the model on the training data\n    model.fit(X_train, y_train)\n\n    # 4. Make predictions on the test set\n    y_pred = model.predict(X_test)\n\n    # 5. Calculate the accuracy score\n    accuracy = accuracy_score(y_test, y_pred)\n\n    return accuracy\n\n# Example Usage (for local testing):\nif __name__ == \"__main__\":\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    accuracy = evaluate_iris_model(X, y)\n    print(f\"Model accuracy on the test set: {accuracy:.4f}\")",
        "id": "1767169545891iixud0jds",
        "status": "completed",
        "assigned_at": "2025-12-31T08:25:45.891Z"
      },
      {
        "title": "Basic Data Cleaning and Feature Creation",
        "description": "You are given a Pandas DataFrame containing customer data. Your task is to perform two fundamental data preparation steps often required before building machine learning models:\n\n1.  **Handle Missing Values**: The 'Age' column has some missing values (`np.nan`). Fill these missing values with the *median* age of all customers.\n2.  **Create a New Feature**: Create a new column called 'Engagement_Score' by multiplying 'Purchases' by 'Website_Visits'. This represents a simple combined metric of customer engagement that could be useful for modeling.",
        "starter_code": "import pandas as pd\nimport numpy as np\n\ndata = {\n    'CustomerID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'Age': [28, 35, np.nan, 42, 29, 50, np.nan, 31, 24, 38],\n    'Purchases': [5, 12, 8, 20, 3, 15, 7, 10, 2, 18],\n    'Website_Visits': [10, 25, 15, 40, 8, 30, 12, 20, 5, 35]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# --- Your code here ---\n# 1. Fill missing 'Age' values with the median\n# 2. Create 'Engagement_Score' = Purchases * Website_Visits\n\n# print(\"\\nProcessed DataFrame:\")\n# print(df)",
        "solution": "import pandas as pd\nimport numpy as np\n\ndata = {\n    'CustomerID': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'Age': [28, 35, np.nan, 42, 29, 50, np.nan, 31, 24, 38],\n    'Purchases': [5, 12, 8, 20, 3, 15, 7, 10, 2, 18],\n    'Website_Visits': [10, 25, 15, 40, 8, 30, 12, 20, 5, 35]\n}\ndf = pd.DataFrame(data)\n\nprint(\"Original DataFrame:\")\nprint(df)\n\n# 1. Fill missing 'Age' values with the median\nmedian_age = df['Age'].median()\ndf['Age'].fillna(median_age, inplace=True)\n\n# 2. Create 'Engagement_Score' = Purchases * Website_Visits\ndf['Engagement_Score'] = df['Purchases'] * df['Website_Visits']\n\nprint(\"\\nProcessed DataFrame:\")\nprint(df)",
        "id": "17671698662994qe0c7js2",
        "status": "completed",
        "assigned_at": "2025-12-31T08:31:06.299Z"
      },
      {
        "title": "First Model: Training and Evaluation",
        "description": "You have a small dataset where you want to predict the number of 'Clicks' on an advertisement based on the 'Impressions' it received. This is a basic regression problem. Your task is to:\n\n1.  **Split Data**: Divide the data into training and testing sets. Use 80% of the data for training and 20% for testing. Set `random_state=42` for reproducibility.\n2.  **Train a Model**: Initialize and train a `LinearRegression` model from Scikit-learn on the *training* data.\n3.  **Make Predictions**: Use the trained model to predict the 'Clicks' for the *test* set.\n4.  **Evaluate Model**: Calculate the Mean Absolute Error (MAE) between the actual 'Clicks' (`y_test`) and your model's predicted 'Clicks' (`y_pred`) on the test set. MAE is a simple metric that tells you the average magnitude of errors in your predictions.",
        "starter_code": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# Sample Data\ndata = {\n    'Impressions': [1000, 1500, 800, 2000, 500, 1200, 1800, 700, 2200, 900],\n    'Clicks': [50, 75, 40, 100, 25, 60, 90, 35, 110, 45]\n}\ndf = pd.DataFrame(data)\n\nX = df[['Impressions']] # Features (input to the model)\ny = df['Clicks']         # Target (what we want to predict)\n\n# --- Your code here ---\n# 1. Split data into X_train, X_test, y_train, y_test\n# 2. Initialize and train a LinearRegression model\n# 3. Make predictions on X_test\n# 4. Calculate MAE\n\n# print(f\"Mean Absolute Error: {mae:.2f}\")\n# print(f\"\\nModel Coefficients: {model.coef_[0]:.2f}\")\n# print(f\"Model Intercept: {model.intercept_:.2f}\")",
        "solution": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# Sample Data\ndata = {\n    'Impressions': [1000, 1500, 800, 2000, 500, 1200, 1800, 700, 2200, 900],\n    'Clicks': [50, 75, 40, 100, 25, 60, 90, 35, 110, 45]\n}\ndf = pd.DataFrame(data)\n\nX = df[['Impressions']] # Features (input to the model)\ny = df['Clicks']         # Target (what we want to predict)\n\n# 1. Split data into X_train, X_test, y_train, y_test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 2. Initialize and train a LinearRegression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# 3. Make predictions on X_test\ny_pred = model.predict(X_test)\n\n# 4. Calculate MAE\nmae = mean_absolute_error(y_test, y_pred)\n\nprint(f\"Mean Absolute Error: {mae:.2f}\")\nprint(f\"\\nModel Coefficients: {model.coef_[0]:.2f}\")\nprint(f\"Model Intercept: {model.intercept_:.2f}\")",
        "id": "1767169866299tz3efp95e",
        "status": "pending",
        "assigned_at": "2025-12-31T08:31:06.299Z"
      },
      {
        "title": "SQL Feature Engineering: Aggregation",
        "description": "You have two relational database tables: `Customers` and `Orders`. Your goal is to use SQL to create a dataset that provides aggregated features for each customer, which could then be used for machine learning tasks like customer segmentation or churn prediction. Specifically, you need to calculate:\n\n1.  The total number of orders made by each customer.\n2.  The average order value for each customer.\n\nThe final output should list `CustomerID`, `TotalOrders`, and `AverageOrderValue` for all customers who have placed at least one order. Order the results by `CustomerID`.",
        "starter_code": "-- Create Customers table\nCREATE TABLE Customers (\n    CustomerID INT PRIMARY KEY,\n    Name VARCHAR(50),\n    Email VARCHAR(100)\n);\n\n-- Insert data into Customers\nINSERT INTO Customers (CustomerID, Name, Email) VALUES\n(101, 'Alice', 'alice@example.com'),\n(102, 'Bob', 'bob@example.com'),\n(103, 'Charlie', 'charlie@example.com');\n\n-- Create Orders table\nCREATE TABLE Orders (\n    OrderID INT PRIMARY KEY,\n    CustomerID INT,\n    OrderDate DATE,\n    OrderValue DECIMAL(10, 2),\n    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\n);\n\n-- Insert data into Orders\nINSERT INTO Orders (OrderID, CustomerID, OrderDate, OrderValue) VALUES\n(1, 101, '2023-01-15', 50.00),\n(2, 101, '2023-02-20', 120.50),\n(3, 102, '2023-03-10', 75.25),\n(4, 101, '2023-04-01', 30.00),\n(5, 103, '2023-05-05', 200.00),\n(6, 102, '2023-06-12', 45.75);\n\n-- --- Your SQL query here ---\n-- Should output CustomerID, TotalOrders, AverageOrderValue\n",
        "solution": "-- Create Customers table\nCREATE TABLE Customers (\n    CustomerID INT PRIMARY KEY,\n    Name VARCHAR(50),\n    Email VARCHAR(100)\n);\n\n-- Insert data into Customers\nINSERT INTO Customers (CustomerID, Name, Email) VALUES\n(101, 'Alice', 'alice@example.com'),\n(102, 'Bob', 'bob@example.com'),\n(103, 'Charlie', 'charlie@example.com');\n\n-- Create Orders table\nCREATE TABLE Orders (\n    OrderID INT PRIMARY KEY,\n    CustomerID INT,\n    OrderDate DATE,\n    OrderValue DECIMAL(10, 2),\n    FOREIGN KEY (CustomerID) REFERENCES Customers(CustomerID)\n);\n\n-- Insert data into Orders\nINSERT INTO Orders (OrderID, CustomerID, OrderDate, OrderValue) VALUES\n(1, 101, '2023-01-15', 50.00),\n(2, 101, '2023-02-20', 120.50),\n(3, 102, '2023-03-10', 75.25),\n(4, 101, '2023-04-01', 30.00),\n(5, 103, '2023-05-05', 200.00),\n(6, 102, '2023-06-12', 45.75);\n\n-- Solution SQL query\nSELECT\n    c.CustomerID,\n    COUNT(o.OrderID) AS TotalOrders,\n    AVG(o.OrderValue) AS AverageOrderValue\nFROM\n    Customers c\nJOIN\n    Orders o ON c.CustomerID = o.CustomerID\nGROUP BY\n    c.CustomerID\nORDER BY\n    c.CustomerID;\n",
        "language": "sql", "id": "1767169866299x4lia0338",
        "status": "completed",
        "assigned_at": "2025-12-31T08:31:06.299Z"
      },
      {
        "title": "Your First Greeting Web App",
        "description": "Welcome to web development! Your first task is to create a very basic web application using Flask, a Python web framework. This app will simply display a personalized greeting on a web page.\n\n**Setup Instructions (for all tasks):**\n1.  **Install Flask:** If you haven't already, open your terminal/command prompt and run `pip install Flask`.\n2.  **Project Structure:** Create a new directory for your project (e.g., `my_webapp`). Inside `my_webapp`, create `app.py` and a sub-directory named `templates`.\n3.  **Run:** After writing your code, save `app.py` and place your HTML files in the `templates` directory. Then, from your `my_webapp` directory in the terminal, run `python app.py`. Flask will provide a URL (usually `http://127.0.0.1:5000/`) that you can open in your browser.\n\n**Task Requirements:**\n*   Set up a Flask application in `app.py`.\n*   Create a route for the root URL (`/`).\n*   This route should render an HTML template named `index.html`.\n*   Inside `index.html`, display the message \"Hello, [Your Name]! Welcome to your first web app.\"\n*   Pass the name (e.g., \"World\") from your Python Flask route to the `index.html` template using Jinja2 templating (Flask's default templating engine).\n*   Ensure the HTML page has a proper title (e.g., \"Greeting App\").",
        "starter_code": "### app.py\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    # TODO 1: Define a variable for the name (e.g., 'World')\n    # TODO 2: Render the 'index.html' template, passing the name variable to it\n    pass\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/index.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Greeting App</title>\n</head>\n<body>\n    <!-- TODO 1: Use a Jinja2 variable here to display the greeting message -->\n    <!-- Example: <h1>Hello, {{ your_variable_name }}! Welcome to your first web app.</h1> -->\n</body>\n</html>",
        "solution": "### app.py\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    name = \"World\" # You can change this name to anything you like\n    return render_template('index.html', user_name=name)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/index.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Greeting App</title>\n</head>\n<body>\n    <h1>Hello, {{ user_name }}! Welcome to your first web app.</h1>\n    <p>This page was rendered using Flask and Jinja2.</p>\n</body>\n</html>",
        "id": "176717035606233a0wt1ox",
        "status": "pending",
        "assigned_at": "2025-12-31T08:39:16.062Z"
      },
      {
        "title": "Simple Message Submission Form",
        "description": "Now that you've displayed static content, let's make your web app interactive! This task focuses on handling user input using HTML forms and different HTTP methods (GET and POST).\n\n**Task Requirements:**\n*   Extend your existing Flask application (or create a new `app.py` if you prefer to keep tasks separate).\n*   Create a new route, `/submit_message`, that handles both `GET` and `POST` requests.\n*   **For `GET` requests to `/submit_message`:**\n    *   Render an HTML template named `form.html`.\n    *   `form.html` should contain a simple HTML form with:\n        *   A `textarea` input field for the user to type a \"message\".\n        *   A submit button.\n        *   The form's `method` should be `POST` and its `action` should point back to `/submit_message`.\n*   **For `POST` requests to `/submit_message`:**\n    *   Retrieve the submitted message from the form data using Flask's `request.form` object.\n    *   Render an HTML template named `message.html`.\n    *   `message.html` should display the message that the user submitted (e.g., \"You submitted: [Your Message]\").\n    *   Include a link on `message.html` to go back to the submission form.",
        "starter_code": "### app.py\nfrom flask import Flask, render_template, request, redirect, url_for\n\napp = Flask(__name__)\n\n# (Optional: Add your Task 1 home route here if you want to keep it)\n# @app.route('/')\n# def home():\n#     return \"Welcome! Navigate to /submit_message to start.\"\n\n@app.route('/submit_message', methods=['GET', 'POST'])\ndef submit_message():\n    if request.method == 'POST':\n        # TODO 1: Get the 'message' data from the submitted form.\n        # HINT: Use `request.form.get('input_field_name')`\n        user_message = None # Replace None with your logic\n        # TODO 2: Render 'message.html', passing the user_message to it.\n        return \"\" # Replace with render_template call\n    else: # GET request\n        # TODO 3: Render 'form.html' to display the input form.\n        return \"\" # Replace with render_template call\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/form.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Submit Message</title>\n</head>\n<body>\n    <h1>Submit Your Message</h1>\n    <form method=\"POST\" action=\"/submit_message\">\n        <label for=\"message_input\">Your Message:</label><br>\n        <!-- TODO 1: Add a <textarea> element with name=\"message\" -->\n        <!-- TODO 2: Add a submit button -->\n    </form>\n</body>\n</html>\n\n### templates/message.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Message Received</title>\n</head>\n<body>\n    <h1>Message Received!</h1>\n    <!-- TODO 1: Display the submitted message here using a Jinja2 variable -->\n    <!-- TODO 2: Add a link back to the /submit_message form -->\n</body>\n</html>",
        "solution": "### app.py\nfrom flask import Flask, render_template, request, redirect, url_for\n\napp = Flask(__name__)\n\n@app.route('/')\ndef home():\n    return \"Welcome! Navigate to /submit_message to submit a message.\"\n\n@app.route('/submit_message', methods=['GET', 'POST'])\ndef submit_message():\n    if request.method == 'POST':\n        user_message = request.form.get('message') # Retrieve data from 'message' textarea\n        return render_template('message.html', message=user_message)\n    else: # GET request\n        return render_template('form.html')\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/form.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Submit Message</title>\n</head>\n<body>\n    <h1>Submit Your Message</h1>\n    <form method=\"POST\" action=\"/submit_message\">\n        <label for=\"message_input\">Your Message:</label><br>\n        <textarea id=\"message_input\" name=\"message\" rows=\"4\" cols=\"50\" required></textarea><br><br>\n        <input type=\"submit\" value=\"Submit Message\">\n    </form>\n</body>\n</html>\n\n### templates/message.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Message Received</title>\n</head>\n</head>\n<body>\n    <h1>Message Received!</h1>\n    <p>You submitted: <strong>{{ message }}</strong></p>\n    <p><a href=\"/submit_message\">Submit another message</a></p>\n</body>\n</html>",
        "id": "1767170356062mgvfx6c6z",
        "status": "pending",
        "assigned_at": "2025-12-31T08:39:16.062Z"
      },
      {
        "title": "Simple Product Catalog Display",
        "description": "Leveraging your Python and SQL background, you often work with structured data. This task bridges that experience to web development by displaying a list of items (like products) from a Python data structure on a web page.\n\n**Task Requirements:**\n*   In your `app.py`, define a Python list of dictionaries. Each dictionary should represent a \"product\" and have keys like `id`, `name`, `price`, and `description`. Create at least 3-5 sample products.\n*   Create a Flask route, `/products`, that will display this product catalog.\n*   This route should render an HTML template named `products.html`.\n*   `products.html` should use a Jinja2 `for` loop to iterate over the list of products passed from your Flask app.\n*   For each product, display its `name`, `price`, and `description` in a readable format (e.g., within an unordered list or a simple div structure). Feel free to add minimal inline CSS for better presentation.",
        "starter_code": "### app.py\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n# Sample product data - this would typically come from a database\nsample_products = [\n    # TODO 1: Add dictionaries here, each representing a product.\n    # Example: {'id': 1, 'name': 'Item A', 'price': 10.99, 'description': 'A fantastic item.'},\n    #          {'id': 2, 'name': 'Item B', 'price': 20.50, 'description': 'Even better than Item A.'}\n]\n\n@app.route('/')\ndef home():\n    return \"Welcome! Navigate to /products to see our catalog.\"\n\n@app.route('/products')\ndef product_list():\n    # TODO 2: Render the 'products.html' template.\n    # TODO 3: Pass the `sample_products` list to the template under a variable name (e.g., 'products').\n    return \"\" # Replace with render_template call\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/products.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Product Catalog</title>\n</head>\n<body>\n    <h1>Our Products</h1>\n    <div class=\"product-list\">\n        <!-- TODO 1: Use a Jinja2 `for` loop to iterate over the 'products' variable -->\n        <!-- TODO 2: Inside the loop, display each product's name, price, and description -->\n        <!-- Example structure for one product: -->\n        <!-- <div>\n                <h3>{{ product.name }}</h3>\n                <p>Price: ${{ product.price }}</p>\n                <p>{{ product.description }}</p>\n             </div> -->\n    </div>\n</body>\n</html>",
        "solution": "### app.py\nfrom flask import Flask, render_template\n\napp = Flask(__name__)\n\n# Sample product data - this would typically come from a database query in a real application\nsample_products = [\n    {'id': 1, 'name': 'Wireless Mouse', 'price': 25.99, 'description': 'Ergonomic mouse with long battery life.'},\n    {'id': 2, 'name': 'Mechanical Keyboard', 'price': 89.99, 'description': 'RGB backlit keyboard with tactile switches.'},\n    {'id': 3, 'name': 'Monitor (27-inch)', 'price': 199.50, 'description': 'Full HD IPS display for vibrant colors.'},\n    {'id': 4, 'name': 'Webcam (1080p)', 'price': 45.00, 'description': 'High-definition webcam for video calls.'}\n]\n\n@app.route('/')\ndef home():\n    return \"Welcome! Navigate to /products to see our catalog.\"\n\n@app.route('/products')\ndef product_list():\n    return render_template('products.html', products=sample_products)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n### templates/products.html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Product Catalog</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 20px; background-color: #f4f4f4; }\n        h1 { color: #333; text-align: center; }\n        .product-list { max-width: 800px; margin: 20px auto; display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; }\n        .product-item { background-color: #fff; border: 1px solid #ddd; border-radius: 8px; padding: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }\n        .product-item h3 { margin-top: 0; color: #0056b3; font-size: 1.5em; }\n        .product-item p { margin-bottom: 5px; line-height: 1.5; color: #666; }\n        .product-item .price { font-weight: bold; color: #28a745; font-size: 1.2em; }\n    </style>\n</head>\n<body>\n    <h1>Our Products</h1>\n    <div class=\"product-list\">\n        {% if products %}\n            {% for product in products %}\n                <div class=\"product-item\">\n                    <h3>{{ product.name }}</h3>\n                    <p class=\"price\">Price: ${{ \"%.2f\"|format(product.price) }}</p>\n                    <p><em>{{ product.description }}</em></p>\n                </div>\n            {% endfor %}\n        {% else %}\n            <p>No products found.</p>\n        {% endif %}\n    </div>\n</body>\n</html>",
        "id": "1767170356062lwh1qx9i2",
        "status": "pending",
        "assigned_at": "2025-12-31T08:39:16.062Z"
      },
      {
        "title": "Simulated Cloud VM Manager",
        "description": "You are building a basic command-line tool to manage simulated Virtual Machines (VMs) in a cloud environment. Implement Python functions to perform the following operations: create a new VM, list all existing VMs, and delete a VM. Each VM should have a unique `id`, a `name`, and an initial `status` (e.g., 'running'). The data should be stored in memory (e.g., a global dictionary), simulating a database or cloud API.",
        "starter_code": "import uuid\n\n# In-memory storage for VMs: {vm_id: {'name': '...', 'status': '...'}}\n# For a real application, this would interact with a database or cloud API.\n_vms_store = {}\n\ndef create_vm(name: str) -> dict:\n    \"\"\"\n    Creates a new simulated VM and adds it to the store.\n    Generates a unique ID for the VM.\n    Initial status should be 'running'.\n\n    Args:\n        name (str): The name of the VM.\n\n    Returns:\n        dict: The dictionary representing the created VM, including its ID.\n    \"\"\"\n    pass\n\ndef list_vms() -> dict:\n    \"\"\"\n    Lists all simulated VMs currently in the store.\n\n    Returns:\n        dict: A dictionary where keys are VM IDs and values are VM details.\n    \"\"\"\n    pass\n\ndef delete_vm(vm_id: str) -> bool:\n    \"\"\"\n    Deletes a simulated VM from the store given its ID.\n\n    Args:\n        vm_id (str): The unique ID of the VM to delete.\n\n    Returns:\n        bool: True if the VM was deleted successfully, False otherwise (e.g., VM not found).\n    \"\"\"\n    pass\n",
        "solution": "import uuid\n\n# In-memory storage for VMs: {vm_id: {'name': '...', 'status': '...'}}\n# For a real application, this would interact with a database or cloud API.\n_vms_store = {}\n\ndef create_vm(name: str) -> dict:\n    \"\"\"\n    Creates a new simulated VM and adds it to the store.\n    Generates a unique ID for the VM.\n    Initial status should be 'running'.\n\n    Args:\n        name (str): The name of the VM.\n\n    Returns:\n        dict: The dictionary representing the created VM, including its ID.\n    \"\"\"\n    vm_id = str(uuid.uuid4())\n    vm_details = {\"id\": vm_id, \"name\": name, \"status\": \"running\"}\n    _vms_store[vm_id] = vm_details\n    return vm_details\n\ndef list_vms() -> dict:\n    \"\"\"\n    Lists all simulated VMs currently in the store.\n\n    Returns:\n        dict: A dictionary where keys are VM IDs and values are VM details.\n    \"\"\"\n    return _vms_store\n\ndef delete_vm(vm_id: str) -> bool:\n    \"\"\"\n    Deletes a simulated VM from the store given its ID.\n\n    Args:\n        vm_id (str): The unique ID of the VM to delete.\n\n    Returns:\n        bool: True if the VM was deleted successfully, False otherwise (e.g., VM not found).\n    \"\"\"\n    if vm_id in _vms_store:\n        del _vms_store[vm_id]\n        return True\n    return False\n",
        "id": "1767172335956r9zj0atrw",
        "status": "pending",
        "assigned_at": "2025-12-31T09:12:15.957Z"
      },
      {
        "title": "Basic Service Config Parser",
        "description": "In DevOps, applications often rely on external configuration files. For this task, you need to implement a Python function that parses a simple configuration string (simulating file content). The configuration uses a `key=value` format, with each setting on a new line. Your function should read this string and return a dictionary where keys are configuration names and values are their respective settings. Handle empty lines and comments (lines starting with `#`).",
        "starter_code": "def parse_config_string(config_content: str) -> dict:\n    \"\"\"\n    Parses a multi-line configuration string into a dictionary.\n    Each line is expected to be in 'key=value' format.\n    Ignores empty lines and lines starting with '#'.\n\n    Args:\n        config_content (str): A string containing the configuration.\n\n    Returns:\n        dict: A dictionary representing the parsed configuration.\n              Returns an empty dictionary if the input is empty or contains no valid config.\n    \"\"\"\n    pass\n",
        "solution": "def parse_config_string(config_content: str) -> dict:\n    \"\"\"\n    Parses a multi-line configuration string into a dictionary.\n    Each line is expected to be in 'key=value' format.\n    Ignores empty lines and lines starting with '#'.\n\n    Args:\n        config_content (str): A string containing the configuration.\n\n    Returns:\n        dict: A dictionary representing the parsed configuration.\n              Returns an empty dictionary if the input is empty or contains no valid config.\n    \"\"\"\n    config = {}\n    lines = config_content.strip().split('\\n')\n    for line in lines:\n        stripped_line = line.strip()\n        if not stripped_line or stripped_line.startswith('#'):\n            continue\n\n        if '=' in stripped_line:\n            key, value = stripped_line.split('=', 1) # Split only on the first '='\n            config[key.strip()] = value.strip()\n    return config\n",
        "id": "1767172335957i1yqms3rv",
        "status": "pending",
        "assigned_at": "2025-12-31T09:12:15.957Z"
      },
      {
        "title": "CI/CD Pipeline Status Aggregator",
        "description": "A core part of DevOps is understanding the state of your Continuous Integration/Continuous Deployment (CI/CD) pipelines. For this task, you'll simulate a simple pipeline with two sequential stages: 'Build' and 'Deploy'. Implement functions to update the status of each stage and a final function to report the overall pipeline status. The possible statuses for each stage are 'PENDING', 'SUCCESS', 'FAILURE'. The overall pipeline status should reflect the state of its stages based on these rules:\n- If the 'build' stage is 'FAILURE', the overall status is 'BUILD_FAILED'.\n- Else if the 'deploy' stage is 'FAILURE', the overall status is 'DEPLOYMENT_FAILED'.\n- Else if both stages are 'SUCCESS', the overall status is 'PIPELINE_SUCCESS'.\n- Otherwise (e.g., any stage is 'PENDING'), the overall status is 'IN_PROGRESS'.",
        "starter_code": "from typing import Dict\n\n# In-memory storage for pipeline stage statuses\n_pipeline_status: Dict[str, str] = {\n    \"build\": \"PENDING\",\n    \"deploy\": \"PENDING\"\n}\n\ndef update_build_status(status: str) -> None:\n    \"\"\"\n    Updates the status of the 'build' stage.\n    Allowed statuses: \"PENDING\", \"SUCCESS\", \"FAILURE\".\n\n    Args:\n        status (str): The new status for the build stage.\n    \"\"\"\n    pass\n\ndef update_deploy_status(status: str) -> None:\n    \"\"\"\n    Updates the status of the 'deploy' stage.\n    Allowed statuses: \"PENDING\", \"SUCCESS\", \"FAILURE\".\n\n    Args:\n        status (str): The new status for the deploy stage.\n    \"\"\"\n    pass\n\ndef get_overall_pipeline_status() -> str:\n    \"\"\"\n    Determines and returns the overall status of the CI/CD pipeline.\n    - If build is 'FAILURE', overall is \"BUILD_FAILED\".\n    - If deploy is 'FAILURE', overall is \"DEPLOYMENT_FAILED\".\n    - If both are 'SUCCESS', overall is \"PIPELINE_SUCCESS\".\n    - Otherwise, overall is \"IN_PROGRESS\".\n\n    Returns:\n        str: The overall status of the pipeline.\n    \"\"\"\n    pass\n",
        "solution": "from typing import Dict\n\n# In-memory storage for pipeline stage statuses\n_pipeline_status: Dict[str, str] = {\n    \"build\": \"PENDING\",\n    \"deploy\": \"PENDING\"\n}\n\ndef update_build_status(status: str) -> None:\n    \"\"\"\n    Updates the status of the 'build' stage.\n    Allowed statuses: \"PENDING\", \"SUCCESS\", \"FAILURE\".\n\n    Args:\n        status (str): The new status for the build stage.\n    \"\"\"\n    if status in [\"PENDING\", \"SUCCESS\", \"FAILURE\"]:\n        _pipeline_status[\"build\"] = status\n    else:\n        raise ValueError(f\"Invalid status for build stage: {status}\")\n\ndef update_deploy_status(status: str) -> None:\n    \"\"\"\n    Updates the status of the 'deploy' stage.\n    Allowed statuses: \"PENDING\", \"SUCCESS\", \"FAILURE\".\n\n    Args:\n        status (str): The new status for the deploy stage.\n    \"\"\"\n    if status in [\"PENDING\", \"SUCCESS\", \"FAILURE\"]:\n        _pipeline_status[\"deploy\"] = status\n    else:\n        raise ValueError(f\"Invalid status for deploy stage: {status}\")\n\ndef get_overall_pipeline_status() -> str:\n    \"\"\"\n    Determines and returns the overall status of the CI/CD pipeline.\n    - If build is 'FAILURE', overall is \"BUILD_FAILED\".\n    - If deploy is 'FAILURE', overall is \"DEPLOYMENT_FAILED\".\n    - If both are 'SUCCESS', overall is \"PIPELINE_SUCCESS\".\n    - Otherwise, overall is \"IN_PROGRESS\".\n\n    Returns:\n        str: The overall status of the pipeline.\n    \"\"\"\n    if _pipeline_status[\"build\"] == \"FAILURE\":\n        return \"BUILD_FAILED\"\n    if _pipeline_status[\"deploy\"] == \"FAILURE\": # Only check deploy if build was not a failure\n        return \"DEPLOYMENT_FAILED\"\n    if _pipeline_status[\"build\"] == \"SUCCESS\" and _pipeline_status[\"deploy\"] == \"SUCCESS\":\n        return \"PIPELINE_SUCCESS\"\n    return \"IN_PROGRESS\"\n",
        "id": "17671723359573hq4na74s",
        "status": "pending",
        "assigned_at": "2025-12-31T09:12:15.957Z"
      },
      {
        "title": "Basic 'Hello World' Web App",
        "description": "Your first task is to create a very basic web application using Python and Flask. The application should have a single route (`/`) that, when accessed, displays the text 'Hello, Web Developer!' in the user's browser. This introduces you to setting up a server, defining routes, and returning simple text responses.",
        "starter_code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    \"\"\"\n    TODO: Return a simple greeting message for the homepage.\n    \"\"\"\n    pass # Placeholder, user will replace this\n\nif __name__ == '__main__':\n    # TODO: Add code to run the Flask application in debug mode\n    pass # Placeholder, user will replace this",
        "solution": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    \"\"\"\n    Returns a simple greeting message for the homepage.\n    \"\"\"\n    return \"Hello, Web Developer!\"\n\nif __name__ == '__main__':\n    app.run(debug=True)",
        "language": "python",
        "id": "1767173714128a3orwv7eh",
        "status": "pending",
        "assigned_at": "2025-12-31T09:35:14.128Z"
      },
      {
        "title": "Displaying Dynamic Data",
        "description": "Building upon the previous task, your goal now is to display dynamic content. Create a new route, `/products`, that shows a list of product items. You should use a Python list of dictionaries to store some sample product data (e.g., 'name' and 'price'). The web application should render this list as an unordered HTML list (`<ul>`) in the browser. This will introduce you to handling data within your Flask app and returning simple HTML.",
        "starter_code": "from flask import Flask\n\napp = Flask(__name__)\n\n# Sample product data (you can add more or modify)\nproducts_data = [\n    {\"name\": \"Laptop\", \"price\": 1200},\n    {\"name\": \"Mouse\", \"price\": 25},\n    {\"name\": \"Keyboard\", \"price\": 75}\n]\n\n@app.route('/')\ndef hello_world():\n    return \"Welcome to our store! Visit /products to see our items.\"\n\n@app.route('/products')\ndef list_products():\n    \"\"\"\n    TODO: Generate an HTML unordered list from `products_data`.\n    TODO: Each list item should display the product name and price.\n    TODO: Return the generated HTML string.\n    \"\"\"\n    pass # Placeholder, user will replace this\n\nif __name__ == '__main__':\n    app.run(debug=True)",
        "solution": "from flask import Flask\n\napp = Flask(__name__)\n\nproducts_data = [\n    {\"name\": \"Laptop\", \"price\": 1200},\n    {\"name\": \"Mouse\", \"price\": 25},\n    {\"name\": \"Keyboard\", \"price\": 75}\n]\n\n@app.route('/')\ndef hello_world():\n    return \"Welcome to our store! Visit /products to see our items.\"\n\n@app.route('/products')\ndef list_products():\n    \"\"\"\n    Generates an HTML unordered list of products from the products_data.\n    \"\"\"\n    html_output = \"<h1>Our Products</h1><ul>\"\n    for product in products_data:\n        html_output += f\"<li>{product['name']} - ${product['price']:.2f}</li>\"\n    html_output += \"</ul>\"\n    return html_output\n\nif __name__ == '__main__':\n    app.run(debug=True)",
        "language": "python",
        "id": "1767173714129u629thi4a",
        "status": "pending",
        "assigned_at": "2025-12-31T09:35:14.129Z"
      },
      {
        "title": "Interactive Greeting Form",
        "description": "For your final task, you'll create a simple interactive web page.\n1.  Create a route `/greet_me` that displays an HTML form with a single text input field for a user's `name` and a submit button.\n2.  When the user enters their name and submits the form, the application should process this input (using a GET request, so the data appears in the URL).\n3.  Finally, it should display a personalized greeting, e.g., 'Hello, [User's Name]!' on the same page. If no name is provided (initial visit or empty submission), it should just display the form.\nThis introduces you to handling basic user input from a web form.",
        "starter_code": "from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/')\ndef homepage():\n    return \"Welcome! Try our greeting form at /greet_me\"\n\n@app.route('/greet_me')\ndef greet_me():\n    \"\"\"\n    TODO: Check if a 'name' parameter is present in the request arguments (request.args).\n    TODO: If a name is present, construct and return a personalized HTML greeting (e.g., \"<h1>Hello, [Name]!</h1>\").\n    TODO: If no name, or on initial visit, display an HTML form with a text input for 'name' and a submit button.\n    \"\"\"\n    pass # Placeholder, user will replace this\n\nif __name__ == '__main__':\n    app.run(debug=True)",
        "solution": "from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/')\ndef homepage():\n    return \"Welcome! Try our greeting form at /greet_me\"\n\n@app.route('/greet_me')\ndef greet_me():\n    \"\"\"\n    Displays a form for a name and then a personalized greeting\n    based on the submitted name via a GET request.\n    \"\"\"\n    user_name = request.args.get('name') # Get the 'name' parameter from the URL query string\n\n    if user_name:\n        # If a name was submitted, display the greeting\n        return f\"<h1>Hello, {user_name}!</h1><p><a href='/greet_me'>Greet someone else?</a></p>\"\n    else:\n        # Otherwise, display the form\n        form_html = \"\"\"\n        <h1>Greet Yourself!</h1>\n        <form action=\"/greet_me\" method=\"get\">\n            <label for=\"name\">Enter your name:</label><br>\n            <input type=\"text\" id=\"name\" name=\"name\" placeholder=\"Your Name\"><br><br>\n            <input type=\"submit\" value=\"Greet Me\">\n        </form>\n        \"\"\"\n        return form_html\n\nif __name__ == '__main__':\n    app.run(debug=True)",
        "language": "python",
        "id": "1767173714129gneuyj9n0",
        "status": "pending",
        "assigned_at": "2025-12-31T09:35:14.129Z"
      }
    ],
    "progress": {
      "streak": 1,
      "completed_tasks": 8,
      "last_active": "2025-12-31T08:25:40.152Z"
    }
  }
}
